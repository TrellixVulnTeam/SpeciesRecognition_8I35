{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Linear probe with hyperparameter sweep for CLIP models. Approach based on https://github.com/openai/CLIP."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn import metrics\n",
                "import os\n",
                "import clip\n",
                "import torch\n",
                "import wandb\n",
                "import utils as uu\n",
                "\n",
                "import numpy as np\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision.datasets import CIFAR100\n",
                "from torch.utils.data.sampler import SubsetRandomSampler\n",
                "from torchvision import transforms\n",
                "\n",
                "from tqdm import tqdm\n",
                "\n",
                "from datasetCUB.Cub_class.class_cub import Cub2011\n",
                "from datasetCUB.transformations import label_transformation as lt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "clip.available_models()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#choose a clip model!\n",
                "model_architecture = 'ViT-B/32'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_name = model_architecture.replace('/', '-')\n",
                "PROJECT_NAME=\"Hyperparameter-Tuning-ViT\"\n",
                "RUN_NAME = \"Linear-Probe-\" + model_name "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the model\n",
                "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
                "model, preprocess = clip.load(model_architecture, device)\n",
                "print(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Alternatively, you can load a model that has been pre-trained on imageNet with a resnet50. (no CLIP-model)\n",
                "\n",
                "# model_architecture = 'resnet50'\n",
                "\n",
                "# preprocess = transforms.Compose([\n",
                "#     transforms.Resize(256),\n",
                "#     transforms.CenterCrop(224),\n",
                "#     transforms.ToTensor(),\n",
                "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
                "# ])\n",
                "# model = torch.hub.load('pytorch/vision:v0.8.0', model_architecture, pretrained=True) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "root = 'path/to/repository/SpeciesRecognition'\n",
                "cub_root = uu.get_root_CUB(root)\n",
                "train = Cub2011(cub_root, train=True, transform_image=preprocess, label_mapping = False)\n",
                "test = Cub2011(cub_root, train=False, transform_image=preprocess, label_mapping = False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get dataset split\n",
                "batch_size = 16\n",
                "validation_split = .2\n",
                "shuffle_dataset = True\n",
                "random_seed = 42\n",
                "\n",
                "# Creating data indices for training and validation splits:\n",
                "dataset_size = len(train)\n",
                "indices = list(range(dataset_size))\n",
                "split = int(np.floor(validation_split * dataset_size))\n",
                "if shuffle_dataset :\n",
                "    np.random.seed(random_seed)\n",
                "    np.random.shuffle(indices)\n",
                "train_indices, val_indices = indices[split:], indices[:split]\n",
                "\n",
                "# Creating PT data samplers and loaders:\n",
                "train_sampler = SubsetRandomSampler(train_indices)\n",
                "valid_sampler = SubsetRandomSampler(val_indices)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_features(dataset,sampler):\n",
                "    all_features = []\n",
                "    all_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(DataLoader(dataset, batch_size=100, sampler=sampler)):\n",
                "            features = model.encode_image(images.to(device))\n",
                "\n",
                "            all_features.append(features)\n",
                "            all_labels.append(labels)\n",
                "            a = torch.cat(all_features).cpu().numpy()\n",
                "            b = torch.cat(all_labels).cpu().numpy()\n",
                "\n",
                "    return a, b\n",
                "\n",
                "# Calculate the image features\n",
                "train_features, train_labels = get_features(train, train_sampler)\n",
                "val_features, val_labels = get_features(train, valid_sampler )\n",
                "test_features, test_labels = get_features(test, None)\n",
                "all_train_features, all_train_labels = get_features(train, None)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_features, train_labels = 0,0\n",
                "val_features, val_labels = 0,0\n",
                "test_features, test_labels = 0,0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Hpyerparametersearch\n",
                "SWEEP = True\n",
                "\n",
                "# fix parameters\n",
                "random_state = 0 \n",
                "max_iter = 1000 \n",
                "verbose = 1\n",
                "\n",
                "# hyperparametersearch for parameter C via wandb\n",
                "c_min = 3.0\n",
                "c_max = 4.0\n",
                "\n",
                "if SWEEP:\n",
                "    sweep_config = {\n",
                "        'method': 'random'\n",
                "        }\n",
                "    metric = {\n",
                "        'name': 'val_accuracy',\n",
                "        'goal': 'maximize'\n",
                "    }\n",
                "    parameters_dict = {\n",
                "        'random_state': {\n",
                "            'value': random_state\n",
                "            },\n",
                "        'C': {\n",
                "            'min': c_min,\n",
                "            'max': c_max \n",
                "            },\n",
                "        'max_iter': {\n",
                "              'value': max_iter\n",
                "            },\n",
                "        'verbose':{\n",
                "            'value': verbose\n",
                "            },\n",
                "        }\n",
                "\n",
                "    sweep_config['parameters'] = parameters_dict\n",
                "    sweep_config['metric'] = metric\n",
                "    sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# run hyperparameter tuning with Wandb\n",
                "\n",
                "cross_val = True # if False: validation on own split\n",
                "\n",
                "def train(config=None):\n",
                "    with wandb.init(config=config):\n",
                "        config = wandb.config\n",
                "        classifier = LogisticRegression(random_state=config.random_state, C=config.C, max_iter=config.max_iter, verbose=config.verbose)\n",
                "    \n",
                "        if cross_val == True:\n",
                "            predicted = cross_val_predict(classifier, all_train_features, all_train_labels, cv=10)\n",
                "            accuracy = metrics.accuracy_score(all_train_labels, predicted)\n",
                "        else: # valiation on own split \n",
                "            classifier.fit(train_features, train_labels)\n",
                "            predictions = classifier.predict(val_features)\n",
                "            accuracy = np.mean((val_labels == predictions).astype(np.float)) * 100.        \n",
                "        \n",
                "        wandb.log({\"val_accuracy\": accuracy})\n",
                "\n",
                "if SWEEP:\n",
                "    wandb.agent(sweep_id, train, count=30)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train classifier\n",
                "\n",
                "random_state = 0 \n",
                "C = 3.59 # choose hyperparameter from sweep\n",
                "max_iter = 1000 \n",
                "verbose = 1\n",
                "\n",
                "run = wandb.init(project=PROJECT_NAME, job_type=\"inference\", name=RUN_NAME)\n",
                "\n",
                "classifier = LogisticRegression(random_state=random_state, C=C, max_iter=max_iter, verbose=verbose)\n",
                "classifier.fit(all_train_features, all_train_labels)\n",
                "predictions = classifier.predict(test_features)\n",
                "accuracy = np.mean((test_labels == predictions).astype(np.float64)) * 100.\n",
                "print(f\"Accuracy = {accuracy:.3f}\")\n",
                "\n",
                "wandb.finish()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "8f1ab7bd00305abf563783cf886ad48376bdebc4e6c4cabca0193d1c2aa508f4"
        },
        "kernelspec": {
            "display_name": "Python 3.7.11 ('MA')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
